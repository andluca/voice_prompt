# Voice-to-Claude Agent Skills

This file provides Claude Code with context and best practices for building this voice-to-text tool.

## Project Context

Building a cross-platform voice-to-text tool for seamless Claude Code prompting. Uses local Whisper large-v3 model for high-accuracy transcription without API costs. Must work on both Linux (Arch-based) and Windows.

## Key Technical Decisions

### Why faster-whisper over openai-whisper
- 4x faster inference (CTranslate2 backend)
- Lower memory footprint
- Same accuracy, better performance
- Active maintenance and GPU optimization

### Why large-v3 model
- Best accuracy for technical vocabulary
- User has 200GB free disk space
- Worth 3GB for near-perfect transcription
- Handles technical terms: Harbor, Stagehand, epic-webdev-bench, Claude Code

### Why pynput over keyboard library
- Better cross-platform support
- Global hotkey monitoring without admin/root
- More reliable keyboard simulation
- Active maintenance

## Architecture Principles

### Single-Threaded with Async Where Needed
- Hotkey listener runs in background thread (required by pynput)
- Main thread handles recording and transcription (blocking is OK)
- Don't over-engineer with async/await unless necessary

### Lazy Model Loading
- Load Whisper model on first use, not at startup
- Keep model in memory for subsequent uses
- This makes startup instant, first transcription pays the cost

### Clean Separation of Concerns
```
HotkeyManager    → Only handles keyboard events
AudioRecorder    → Only captures audio
WhisperTranscriber → Only does transcription
TextOutputter    → Only handles text output
ConfigManager    → Only manages configuration
```

Each component should be testable independently.

## Implementation Guidelines

### Cross-Platform Audio Recording

**Use sounddevice library:**
```python
import sounddevice as sd
import numpy as np

# This works identically on Linux and Windows
audio = sd.rec(int(duration * samplerate), 
               samplerate=samplerate, 
               channels=1, 
               dtype='int16')
sd.wait()  # Wait for recording to finish
```

**Avoid**: pyaudio (requires complex PortAudio setup), wave (low-level)

### Global Hotkey Registration

**Use pynput correctly:**
```python
from pynput import keyboard

# Combination hotkey
def on_activate():
    print("Hotkey activated!")

# Use pynput's Key constants
hotkey = keyboard.HotKey(
    keyboard.HotKey.parse('<ctrl>+<shift>+v'),
    on_activate
)

# Global listener
with keyboard.Listener(
    on_press=lambda k: hotkey.press(keyboard.KeyCode.from_vk(k)),
    on_release=lambda k: hotkey.release(keyboard.KeyCode.from_vk(k))
) as listener:
    listener.join()
```

**Critical**: Must run in separate thread or will block

### Whisper Model Setup

**Use faster-whisper API:**
```python
from faster_whisper import WhisperModel

# Load once, reuse many times
model = WhisperModel(
    "large-v3",
    device="cuda" if torch.cuda.is_available() else "cpu",
    compute_type="float16" if torch.cuda.is_available() else "int8",
    download_root="~/.cache/whisper"  # Standard cache location
)

# Transcribe with options
segments, info = model.transcribe(
    audio_path,
    language="en",  # Hint for better accuracy
    beam_size=5,    # Default, good balance
    vad_filter=True  # Remove silence
)

# Extract text
text = " ".join([segment.text for segment in segments])
```

**Model auto-download**: First run downloads ~3GB, subsequent runs are instant

### Keyboard Simulation

**Type into active window:**
```python
from pynput.keyboard import Controller

keyboard = Controller()

# Type character by character (looks natural)
for char in text:
    keyboard.type(char)
    
# Or instant (no delay)
keyboard.type(text)
```

**Handle special characters:**
```python
# Newlines, tabs, etc.
if char == '\n':
    keyboard.press(Key.enter)
    keyboard.release(Key.enter)
```

### Configuration Management

**Use PyYAML with defaults:**
```python
import yaml
from pathlib import Path

def load_config(config_path: Path) -> dict:
    defaults = {
        "hotkeys": {"record": "ctrl+shift+v"},
        "audio": {"sample_rate": 16000},
        # ... more defaults
    }
    
    if config_path.exists():
        with open(config_path) as f:
            user_config = yaml.safe_load(f)
        # Merge user config over defaults
        return {**defaults, **user_config}
    
    return defaults
```

**Config location**: `~/.voice-to-claude/config.yaml` (XDG-compliant on Linux)

## Error Handling Patterns

### Graceful Degradation

**Microphone not available:**
```python
try:
    audio = sd.rec(...)
except sd.PortAudioError:
    logger.error("Microphone not found")
    notify_user("No microphone detected. Please check connections.")
    # Don't crash, continue listening for hotkey
```

**Model loading fails:**
```python
try:
    model = WhisperModel(...)
except Exception as e:
    logger.error(f"Failed to load model: {e}")
    notify_user("Model download required. Run 'voice-prompt install-model'")
    # Provide fallback or exit gracefully
```

**Transcription fails:**
```python
try:
    text = transcribe(audio_path)
except Exception as e:
    logger.error(f"Transcription failed: {e}")
    # Save audio file for debugging
    debug_path = Path("~/.voice-to-claude/failed") / f"{timestamp}.wav"
    shutil.copy(audio_path, debug_path)
    notify_user("Transcription failed. Audio saved for debugging.")
```

### User Notifications

**Cross-platform notifications:**
```python
# Simple: print to console if running in terminal
# Better: Use plyer library for desktop notifications
from plyer import notification

notification.notify(
    title="Voice Prompt",
    message="Recording started...",
    timeout=2
)
```

## Testing Approach

### Unit Tests with Mocking

**Test recorder without actual microphone:**
```python
from unittest.mock import patch, MagicMock

@patch('sounddevice.rec')
def test_recorder(mock_rec):
    mock_rec.return_value = np.zeros((16000, 1))
    recorder = AudioRecorder()
    audio = recorder.record(duration=1.0)
    assert audio is not None
    mock_rec.assert_called_once()
```

### Integration Tests with Real Audio

**Use pre-recorded test samples:**
```python
def test_transcription_accuracy():
    test_audio = Path("tests/samples/technical_speech.wav")
    transcriber = WhisperTranscriber()
    text = transcriber.transcribe(test_audio)
    
    # Check for key technical terms
    assert "harbor" in text.lower()
    assert "stagehand" in text.lower()
```

## Performance Considerations

### Model Loading Time
- Cold start: ~5-10 seconds (loading model into memory)
- Warm transcription: ~3-5 seconds per 30s audio
- Keep model in memory to avoid reload cost

### Memory Usage
- Model in RAM: ~3GB (large-v3)
- Audio buffer: ~1MB per minute of recording
- Total footprint: ~3-4GB when running

### CPU vs GPU
- GPU: 3-4x faster, requires CUDA
- CPU: Still usable, int8 quantization helps
- Auto-detect and fallback to CPU

## Common Pitfalls to Avoid

### 1. Blocking the Main Thread
❌ **Wrong:**
```python
# This blocks everything
with keyboard.Listener(...) as listener:
    listener.join()  # Blocks here forever
    # Nothing below runs!
```

✅ **Right:**
```python
# Run listener in background thread
listener = keyboard.Listener(...)
listener.start()

# Main thread continues
while running:
    # Handle other events
    time.sleep(0.1)
```

### 2. Not Cleaning Up Temp Files
❌ **Wrong:**
```python
audio_file = "/tmp/recording.wav"
save_audio(audio, audio_file)
text = transcribe(audio_file)
# File left on disk!
```

✅ **Right:**
```python
with tempfile.NamedTemporaryFile(suffix='.wav', delete=True) as f:
    save_audio(audio, f.name)
    text = transcribe(f.name)
# File automatically deleted
```

### 3. Not Handling Model Download
❌ **Wrong:**
```python
# Assumes model already downloaded
model = WhisperModel("large-v3")
```

✅ **Right:**
```python
try:
    print("Loading model (may download on first run)...")
    model = WhisperModel("large-v3", download_root=cache_dir)
    print("Model ready!")
except Exception as e:
    print(f"Failed to load model: {e}")
    print("Run 'voice-prompt download-model' first")
```

### 4. Hardcoding Platform-Specific Paths
❌ **Wrong:**
```python
config_path = "~/.voice-to-claude/config.yaml"  # Unix only
```

✅ **Right:**
```python
from pathlib import Path
config_path = Path.home() / ".voice-to-claude" / "config.yaml"
# Works on both Windows and Unix
```

## Development Workflow

### 1. Start with Core Components
Build in this order:
1. ConfigManager (foundation)
2. AudioRecorder (can test independently)
3. WhisperTranscriber (can test with sample audio)
4. TextOutputter (simple, no dependencies)
5. HotkeyManager (requires manual testing)
6. Main integration (ties everything together)

### 2. Test Each Component
```bash
# Test recorder
python -m voice_prompt.recorder

# Test transcriber
python -m voice_prompt.transcriber tests/sample.wav

# Test outputter
python -m voice_prompt.outputter "Hello World"

# Test full integration
python -m voice_prompt
```

### 3. Handle Platform Differences Early
- Test on Linux first (primary platform)
- Then test on Windows (secondary)
- Document any platform-specific behavior

## Installation Script Guidelines

### Linux setup.sh
```bash
#!/bin/bash
set -e

# Create venv
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create config directory
mkdir -p ~/.voice-to-claude
cp config.yaml.example ~/.voice-to-claude/config.yaml

# Download model (optional, can do on first run)
echo "Download Whisper model now? (3GB) [y/N]"
read -r response
if [[ "$response" =~ ^[Yy]$ ]]; then
    python scripts/download-model.py
fi

# Install systemd service (optional)
echo "Install as system service? [y/N]"
read -r response
if [[ "$response" =~ ^[Yy]$ ]]; then
    ./scripts/install-service.sh
fi

echo "Installation complete! Run: voice-prompt start"
```

### Windows setup.ps1
```powershell
# Similar but Windows-specific
# Use Start-Process for service installation
# Use PowerShell path handling
```

## Documentation Requirements

### README.md Must Include:
1. **Quick start** - 5 lines max to get running
2. **Requirements** - Python version, OS support
3. **Installation** - Step-by-step for both platforms
4. **Usage** - How to use with examples
5. **Configuration** - Explain config.yaml options
6. **Troubleshooting** - Common issues and fixes
7. **Contributing** - How to report bugs/contribute

### Code Comments:
- Explain **why**, not **what** (code shows what)
- Document complex algorithms
- Note platform-specific workarounds
- Link to relevant documentation

## Quality Checklist

Before marking complete:
- [ ] Works on Linux (tested on Omarchy)
- [ ] Works on Windows (tested on Windows 10/11)
- [ ] >95% accuracy on technical speech (tested with samples)
- [ ] <10 second transcription for 60s audio
- [ ] All temp files cleaned up
- [ ] No crashes on common errors (no mic, no model, etc.)
- [ ] Config validation prevents bad values
- [ ] Installation scripts work fresh on both platforms
- [ ] README includes troubleshooting section
- [ ] Code follows PEP 8 style (use ruff or black)

## Additional Resources

### Dependencies Documentation
- faster-whisper: https://github.com/guillaumekln/faster-whisper
- pynput: https://pynput.readthedocs.io/
- sounddevice: https://python-sounddevice.readthedocs.io/
- PyYAML: https://pyyaml.org/wiki/PyYAMLDocumentation

### Whisper Model Details
- Model info: https://github.com/openai/whisper
- large-v3: 1550M parameters, ~3GB
- Optimized for: 96 languages, technical vocabulary
- Expected accuracy: 95-98% WER (Word Error Rate)

### Cross-Platform Best Practices
- Use pathlib for paths (not os.path)
- Use platform.system() to detect OS
- Test on both platforms regularly
- Document platform-specific limitations
